---
title: "Análise da precisão"
output: html_notebook
---

```{r}
library(tidyverse)
library(here)
library(modelr)
library(broom)
library(GGally)

theme_set(theme_bw())
```

## Os dados

```{r carrega}

reclamacoes = read_csv(here("data/3-avaliacao-humana/reclamacoes-avaliadas-20190515.csv"))
sentimentos = read_csv(here("data/5-sentimentos/sentimento.csv"))

reclamacoes = reclamacoes %>% mutate(comprimento_reclamacao = str_length(texto))
```

`reclamacoes_l` tem um formato long em vez de wide (explicado [aqui](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/)).

```{r junta}
reclamacoes = reclamacoes %>% 
    left_join(sentimentos, by = "id")

reclamacoes_l = reclamacoes %>%  
    select(-palavras_op30, -palavras_sent, -`Grupo que vai avaliar`) %>% 
    gather(key = "lexico", 
           value = "polaridade", 
           sentimento_op30, sentimento_sent)

reclamacoes_l %>% View()

```

Converte polaridades para escala 1-5

```{r}

# Faça você mesmo. Crie a variável polaridade_normalizada

# Normalização mínimo-máximo
scaling <- function(valor, minimo, maximo) {
  valor = ((valor - minimo)/(maximo - minimo)) * 4 + 1
  return(valor)
}

# Converter satisfação em insatisfação
unsatisfaction <- function(valor){
  valor = (valor - 6) * -1                              
  return(valor)
}

polaridade_minima = min(reclamacoes_l$polaridade)
polaridade_maxima = max(reclamacoes_l$polaridade)

reclamacoes_l = reclamacoes_l %>% 
     group_by(lexico) %>% 
     mutate(polaridade_normalizada = unsatisfaction(scaling(polaridade, polaridade_minima, polaridade_maxima)))

```

Calcula o erro (SSE) por reclamação

```{r}
reclamacoes_l = reclamacoes_l %>% 
    mutate(erro = (insatisfacao - polaridade_normalizada)**2)
```


## EDA

Inicial. Faça os gráficos a mais que achar necessário para entender os dados que temos de resultado. Lembrando de nossa questão: Quão eficazes são os métodos de análise de sentimento baseados em léxicos para estimar o nível de insatisfação de reclamações recebidas pelo reclameaqui do governo federal? Existe um exemplo de EDA no repositório. Uma decisão importante a ser usada é se vamos considerar as avaliações humanas onde houve muita discordância sobre o nível de insatisfação.

###Como avaliar a eficácia dos métodos?  
Uma medida interessante da eficiência desses métodos é calcular a soma dos erros ao quadrado (SSE) considerando o que o método definiu como a polaridade_normalizada e o que a avaliação humana definiu como a insatisfação.

```{r}
reclamacoes %>% 
    ggplot(aes(x = sentimento_op30, y = sentimento_sent)) + 
    geom_abline(slope = 1, intercept = 0, color = "grey") + 
    geom_count(alpha = .7) 
```

```{r}
reclamacoes_l %>% 
    ggplot(aes(x = insatisfacao, y = polaridade_normalizada, group = insatisfacao)) + 
    geom_abline(slope = 1, intercept = 0, color = "grey") + 
    geom_jitter(alpha = .7)  + 
    facet_wrap(~ lexico)

reclamacoes_l %>% 
    ggplot(aes(x = insatisfacao, y = erro, group = insatisfacao)) + 
    geom_jitter(alpha = .5)  +
    # geom_boxplot() + 
    facet_wrap(~ lexico)
```


## Há relação entre o léxico e o erro?

Agora um modelo para responder sua pergunta.

```{r}
#Cria variável dummy para preditor categórico
reclamacoes_l = reclamacoes_l %>% mutate(lexico.dummy = if_else(lexico == "sentimento_sent", 1, 0))
reclamacoes_sent = reclamacoes_l %>% filter(lexico.dummy == 1)
reclamacoes_op30 = reclamacoes_l %>% filter(lexico.dummy == 0)


#Você precisa entender o que fez acima para interpretar sua regressão
#Você pode também criar uma variável dummy para o órgao (se anac ou inss)

reclamacoes_l[12][reclamacoes_l[12]=="sentimento_op30"] <- "OP_30"
reclamacoes_l[12][reclamacoes_l[12]=="sentimento_sent"] <- "SENT"

ggpairs(reclamacoes_l %>% select(erro), 
        ggplot2::aes(colour=lexico), 
        upper = list(continuous=wrap("density", alpha=1), combo="box"),
        lower = list(continuous=wrap("points", alpha=1), combo=wrap("dot", alpha=1)),
        diag = list(continuous=wrap("densityDiag")))

summary(lm(reclamacoes_sent$erro ~ reclamacoes_op30$erro))
```


Regressão simples foi utilizada para analisar se os erros de OP_30 e SENT tem uma associação significativa entre si. 
Os resultados da regressão indicam um modelo com um preditor no formato Erro_SENT = 0.93051 * Erro_OP_30 + 0.27073,
com um intervalo de confiança de 95%. Nele percebemos, através do ponto fixo,
que quando o erro de OP_30 é menor do que 3.8959, este léxico possui maior precisão em relação a SENT.
No entanto, a partir deste valor a situação se inverte. Portanto, chegamos a conclusão de que quanto menor os erros dos léxicos,
mais preciso OP_30 será em relação à SENT. No entanto, SENT se mantém mais preciso do que OP_30 quando a imprecisão supera o valor mencionado.
